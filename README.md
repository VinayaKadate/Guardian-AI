# Sentinel AI

Watch the full project walkthrough:  
https://youtu.be/kyne3G_owR4?si=TSCfTjFPft7LxTdB

---

# ğŸ¤– Offline AI Assistant

A fully containerized **offline AI assistant** built using:

- FastAPI (Backend API)
- Ollama (Local LLM)
- Qdrant (Vector Database)
- LangChain (RAG Pipeline)
- Docker Compose (Service Orchestration)

This project enables document-based question answering using a local LLM without external APIs.

---

# ğŸ“ Project Structure

```
offline-ai-assistant/
â”‚
â”œâ”€â”€ backend/
â”œâ”€â”€ frontend/
â”œâ”€â”€ data/
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

# âš™ï¸ Requirements

- Docker Desktop
- WSL2 (for Windows)
- Minimum 8GB RAM recommended

---

# ğŸš€ Setup Instructions

## 1ï¸âƒ£ Start Services

```bash
docker compose up -d
```

## 2ï¸âƒ£ Pull LLM Model (Required)

```bash
docker exec -it ollama ollama pull llama3
```

Verify:

```bash
docker exec -it ollama ollama list
```

---

# ğŸŒ Access the Application

Frontend:
```
http://localhost:8080
```

Backend API Docs:
```
http://localhost:8000/docs
```

---

# ğŸ“„ How to Use

1. Open `http://localhost:8080`
2. Upload PDF / CSV / Excel documents
3. Ask questions based on uploaded documents
4. The system retrieves relevant context and generates answers locally

---

# ğŸ³ Services

- **ollama** â€“ Local LLM inference  
- **qdrant** â€“ Vector storage  
- **backend** â€“ API and RAG logic  
- **frontend** â€“ Web interface  

---

# ğŸ”„ Reset (If Needed)

```bash
docker compose down -v
docker compose up -d --build
```

---

# ğŸ“Œ Notes

- The system runs fully offline after the model is downloaded.
- Use quantized models for better CPU performance if needed.
